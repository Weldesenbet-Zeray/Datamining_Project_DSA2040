---
title: "Project_1"
author: "Weldesenbet Zeray Aregay"
date: "2025-03-08"
output: html_document
---

## Loading Necessary Packages:

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
library(dplyr)
library(lubridate)

```

## Loading the Original large Data Sets:

```{r}
data_1 <- read.csv("C:\\Users\\Admin\\OneDrive\\Documents\\SS_2025\\DSA2040\\Project_Dataset\\new_retail_data_2.csv")

data_1
```

```{r}
data_2 <-read.csv("C:\\Users\\Admin\\OneDrive\\Documents\\SS_2025\\DSA2040\\Project_Dataset\\retail_data_1.csv")

data_2
```

## Exploring the Columns of each data set:

```{r}
colnames(data_2)
```

```{r}
colnames(data_1)
```

## Sampling the required records from the Original data sets:

```{r}
set.seed(50000)
dataset_1 <- data_1 %>% 
  sample_n(50000)
write.csv(dataset_1,"sampled_dataset_1.csv",row.names=FALSE)
```

```{r}
set.seed(50000)
dataset_2 <- data_2 %>% 
  sample_n(50000)
write.csv(dataset_2,"sampled_dataset_2.csv",row.names=FALSE)
```

## Filtering the Necessary Columns:

```{r}
necessary_columns_1 <- c("Transaction_ID", "Customer_ID", "Date", "Year", "Month", "Time", "Total_Purchases", "Amount", "Total_Amount", "Product_Category", "Product_Brand", "Product_Type", "products", "Payment_Method", "Shipping_Method", "Order_Status", "Ratings", "Customer_Segment", "Feedback")

necessary_columns_2 <- c("customer_id", "age", "gender", "income_bracket", "loyalty_program", "membership_years", "churned", "transaction_id", "transaction_date", "transaction_hour", "day_of_week", "month_of_year", "product_category", "product_name", "quantity", "unit_price", "discount_applied", "payment_method", "store_location", "total_sales", "total_transactions", "total_items_purchased", "avg_items_per_transaction", "purchase_frequency", "avg_transaction_value", "total_discounts_received", "preferred_store", "product_rating", "promotion_type", "promotion_channel", "customer_city", "customer_state", "distance_to_store", "email_subscriptions", "website_visits", "social_media_engagement", "days_since_last_purchase")


```

```{r}
Data_1_sliced <- dataset_1 %>% select(all_of(necessary_columns_1))
Data_2_sliced <- dataset_2 %>% select(all_of(necessary_columns_2))
```

```{r}
Data_1_sliced
```

```{r}
Data_2_sliced
```

# EDA of the Sliced Data sets: 

### Overview of the Data

```{r}
# Check structure and summary of dataset 1
str(Data_1_sliced)
#summary(Data_1_sliced)

```


```{r}
# Check structure and summary of dataset 2
str(Data_2_sliced)
#summary(Data_2_sliced)
```

### Missing Value Analysis

```{r}
colSums(is.na(Data_1_sliced))
```


```{r}
colSums(is.na(Data_2_sliced))
```

#### Handling Missing Values of the Data_1_sliced dataset:
‚úÖ Best Practice
If missing values are <5%, use imputation.
If missing values are between 5% and 30%, decide based on feature importance.
If missing values are >30%, consider removal unless the column is critical.

#### Extracting year from the "Date" column to fill the missing values of "Year" column:
This is because the Date column has no any missing values and since it contains year so this gives us a best way to fill the missing values of this column.

But after the Date column has changed to Date type because of inconvinient format it still contains missing values so We have filled this NA's with the most frequent Date Column.

```{r}
Data_1_sliced$Date <- as.Date(Data_1_sliced$Date, format="%m/%d/%Y")

#Find the Most Frequent (Mode) Date
most_frequent_date <- Data_1_sliced %>%
  filter(!is.na(Date)) %>%
  count(Date) %>%
  arrange(desc(n)) %>%
  slice(1) %>%
  pull(Date)

print(most_frequent_date)  # Check the most frequent date

#Fill NA values in Date column with the most frequent date
Data_1_sliced$Date[is.na(Data_1_sliced$Date)] <- most_frequent_date


Data_1_sliced <- Data_1_sliced %>% 
  mutate(Year=ifelse(is.na(Year),year(Date),Year))   # Fill missing Year values
```




#### Filling the Amount,Total_Purchases and Total_Amount:
By using the mathematical relationship of the three variables we have filled the missing values and when all the three variables are missed we have used the median of the columns. This gives as a best way of filling tje missing values with almost real values/ Optimizing the existing data points.
```{r}

Data_1_sliced <- Data_1_sliced %>%
  mutate(
    # Fill Total_Amount if Amount & Total_Purchases exist
    Total_Amount = ifelse(is.na(Total_Amount) & !is.na(Amount) & !is.na(Total_Purchases), 
                          Amount * Total_Purchases, Total_Amount),
    
    # Fill Amount if Total_Amount & Total_Purchases exist
    Amount = ifelse(is.na(Amount) & !is.na(Total_Amount) & !is.na(Total_Purchases), 
                    Total_Amount / Total_Purchases, Amount),
    
    # Fill Total_Purchases if Total_Amount & Amount exist
    Total_Purchases = ifelse(is.na(Total_Purchases) & !is.na(Total_Amount) & !is.na(Amount), 
                             Total_Amount / Amount, Total_Purchases),
    
    # If all are missing, fill with median
    Amount = ifelse(is.na(Amount), median(Amount, na.rm = TRUE), Amount),
    Total_Purchases = ifelse(is.na(Total_Purchases), median(Total_Purchases, na.rm = TRUE), Total_Purchases),
    Total_Amount = ifelse(is.na(Total_Amount), median(Total_Amount, na.rm = TRUE), Total_Amount)
  )

```


#### Filling the Ratings column:
We have grouped it first by customer segmentation for better accuracy and then It is filled by the mean value of each group for the missed values and lastly is has returned to the orignal dataset format after filled using umgroup() function.
```{r}
Data_1_sliced <- Data_1_sliced %>% 
  group_by(Customer_Segment) %>% 
  mutate(Ratings = ifelse(is.na(Ratings), round(mean(Ratings, na.rm = TRUE)), Ratings)) %>% 
  ungroup()
Data_1_sliced
```


#### üîπ Recovering Missing IDs from Related Columns üõ†Ô∏è
When Customer_ID or Transaction_ID is missing, we try to recover them by using related columns. The logic is:

If a Customer_ID is missing, but the same customer has another row (same name, email, or segment), use the known ID.
If a Transaction_ID is missing, but the same purchase details (date, total amount, product, etc.) exist, use the known transaction.


1Ô∏è‚É£ Filling Missing Customer_IDs Based on Existing Data
üëâ If a customer has at least one known ID, use that ID for missing rows.
It groups similar transactions (Customer_Segment, Date, Total_Amount).
If a Customer_ID is missing, it fills it with the first available ID in that group.

```{r}
Data_1_sliced <- Data_1_sliced %>%
  group_by(Customer_Segment, Date, Total_Amount) %>%  # Group by similar known attributes
  mutate(Customer_ID = ifelse(is.na(Customer_ID), first(na.omit(Customer_ID)), Customer_ID)) %>%
  ungroup()
sum(is.na(Data_1_sliced$Customer_ID))
```


2Ô∏è‚É£ Filling Missing Transaction_IDs for the Same Purchase Details
üëâ If a Transaction_ID is missing, but the same customer made the same purchase, use that known transaction.
```{r}
Data_1_sliced <- Data_1_sliced %>%
  group_by(Customer_ID, Date, Total_Amount, Product_Category) %>%  
  mutate(Transaction_ID = ifelse(is.na(Transaction_ID), first(na.omit(Transaction_ID)), Transaction_ID)) %>%
  ungroup()
sum(is.na(Data_1_sliced$Transaction_ID))
```
If both Customer_ID and Transaction_ID are missing, we need a strategy to infer them intelligently.
For now the are no missed values for both of the columns. 

```{r}
# Assign a new Customer_ID if still missing
Data_1_sliced$Customer_ID[is.na(Data_1_sliced$Customer_ID)] <- 
  max(Data_1_sliced$Customer_ID, na.rm = TRUE) + seq(1, sum(is.na(Data_1_sliced$Customer_ID)))

# Assign a new Transaction_ID if still missing
Data_1_sliced$Transaction_ID[is.na(Data_1_sliced$Transaction_ID)] <- 
  max(Data_1_sliced$Transaction_ID, na.rm = TRUE) + seq(1, sum(is.na(Data_1_sliced$Transaction_ID)))
sum(is.na(Data_1_sliced$Transaction_ID))
sum(is.na(Data_1_sliced$Customer_ID))
```
#### Checking for missing values after applying the techniques:
Since Dataset_2_sliced has no any missing value so we are now free to procide to next stage of EDA.

```{r}
colSums(is.na(Data_1_sliced))
```
```{r}
colSums(is.na(Data_2_sliced))
```

# Part-5) Datamining and Association Rules:

### Convert Data into a Suitable Format
**Step 1:** Prepare Transaction Data
Since ARM algorithms (Apriori & FP-Growth) require transactions in a structured format, we need to convert products into a transactional dataset.

```{r,warning=FALSE,message=FALSE}
# Load necessary libraries
library(arules)
library(arulesViz)
```

```{r}
# Convert products column into transaction format
transactions <- as(split(Data_1_sliced$products, Data_1_sliced$Transaction_ID), "transactions")

# View summary of transactions
summary(transactions)
```
The initial datasets transaction distribution per items:

```{r}
# For Dataset 1
cat("Dataset 1 Transaction Distribution:\n")
table(table(Data_1_sliced$Transaction_ID))

```
```{r}
# For Dataset 2
cat("\nDataset 2 Transaction Distribution:\n")
table(table(Data_2_sliced$transaction_id))
```


**The Sliced datasets have few multi-itemsets transaction as shown above.**
So we have decided to use the big orignal datas transactions so as to create meaningful Association rule mining.
since it accurately contains multi-itemset trabsactions.


```{r}
# For Dataset 1
cat("data 1 Transaction Distribution:\n")
table(table(data_1$Transaction_ID))

```
```{r}
# For Dataset 2
cat("\ndata 2 Transaction Distribution:\n")
table(table(data_2$transaction_id))
```

### As shown above this gives us better ARM possiblity than of using the sliced datasets.

# Converting the data to transactional data for the orignal datasets:
For data_1 we have used:"Transaction_ID" and "products" columns. But For data_2 it is a bit hardet but using the most descriptive columns lets change it to trasactional data.
```{r}
length(unique(data_2$product_name))  # Check unique product names
length(unique(data_2$product_id))  # Check unique product IDs
```
So Since the unique product names are few it is not better option to use it. So We have decided to use the "transaction_id" with "product_id" and "product_category" columns to create a better Association rule mining for the given dataset.

```{r}
data_2$item_label <- paste(data_2$product_category, data_2$product_id, sep="_")
# Convert to transactions
transactions_2 <- as(split(data_2$item_label, data_2$transaction_id), "transactions")

# View summary
summary(transactions_2)
```

### Step-1) Keeping Only Necessary Columns
```{r}
# Select necessary columns
data_1_selected <- data_1[, c("Transaction_ID", "products")]
data_2_selected <- data_2[, c("transaction_id", "product_id", "product_category")]

# Create a combined item label for data_2
data_2_selected$item_label <- paste(data_2_selected$product_category, data_2_selected$product_id, sep="_")

# Drop the original product_id and product_category columns (since we merged them)
data_2_selected <- data_2_selected[, c("transaction_id", "item_label")]

# View first few rows
head(data_1_selected)
```
```{r}
head(data_2_selected)
```


### Step-2) Checking Transaction Size Distribution of Both datasets:
Before applying Association Rule Mining (ARM), we must check if transactions contain multiple items.
```{r}
# Count items per transaction
cat("Dataset 1 Transaction Distribution:\n")
table(table(data_1_selected$Transaction_ID))
```
```{r}
cat("\nDataset 2 Transaction Distribution:\n")
table(table(data_2_selected$transaction_id))
```


### Step 3) Check & Handle Missing Values
We must check for missing values and decide how to handle them.

```{r}
# Check for missing values
cat("Missing values in Dataset 1:\n")
colSums(is.na(data_1_selected))

cat("\nMissing values in Dataset 2:\n")
colSums(is.na(data_2_selected))
```
```{r}
# Remove rows with missing values
data_1_selected <- na.omit(data_1_selected)
data_2_selected <- na.omit(data_2_selected)

# Check for missing values
cat("Missing values in Dataset 1:\n")
colSums(is.na(data_1_selected))

cat("\nMissing values in Dataset 2:\n")
colSums(is.na(data_2_selected))
```


### Step 4) Convert to Transaction Format:
We need to transform the dataset into a transactional format for ARM.

```{r}
# Convert Dataset 1 to transaction format
transactions_1 <- as(split(data_1_selected$products, data_1_selected$Transaction_ID), "transactions")

# Convert Dataset 2 to transaction format
transactions_2 <- as(split(data_2_selected$item_label, data_2_selected$transaction_id), "transactions")

```

```{r}
# View summary
summary(transactions_1)
```

```{r}
summary(transactions_2)

```


###Step 5) Run Association Rule Mining (Apriori Algorithm):
Now We apply Apriori to find frequent itemsets and association rules.But before that lets visualize the most ferquent items so that it can help us to decide the support for the apriori algorithm for each of the datasets.


##### Transaction_1
Lets visualize the item ferequency plot for to decide the threshould support and confidence: using *itemFrequencyPlot(transactions_1, topN = 20)* function in R.

Generates a bar plot displaying the frequency of the top 20 most common items
1)Most Frequently Purchased Items ‚Äì It highlights the products that appear most often in transactions, helping to understand popular items in your dataset.

2)Potential Candidates for Association Rules ‚Äì Frequent items are more likely to appear in strong association rules (e.g., as antecedents or consequents).

3)Threshold for Support ‚Äì If you notice that certain items appear in very few transactions, you may adjust the minimum support (minsup) parameter to filter out rare items and focus on meaningful rules.

4)Business Insights ‚Äì Retailers can use this to identify best-selling products and decide on strategies such as bundling frequently bought items or optimizing stock.
```{r}
itemFrequencyPlot(transactions_1, topN = 20)
```


key takeaways:

Water-related products dominate ‚Äì The most frequent items in the dataset appear to be different types of water (Spring water, Bottled water, Distilled water, etc.). This suggests that water-related products are commonly purchased in transactions.

Mixed product categories ‚Äì Apart from water products, we also see items like Motorola Mob, Google Pixel, Samsung Galaxy (electronics) and Literary fiction, History, Self-help, Horror, Historical fiction (books). This indicates a dataset with diverse product categories.

Frequent items can drive association rules ‚Äì Items that frequently appear together (e.g., different types of water) might have strong association rules. For example, customers buying "Spring water" might also buy "Bottled water," leading to a strong rule like:
{Spring water} ‚Üí {Bottled water}

Unusual combinations ‚Äì The presence of both water-related products and electronics/books in the top 20 items suggests a broad dataset, which may lead to some unexpected association rules. You might want to check for segment-based analysis (e.g., rules within categories).


```{r}
frequent_itemsets <- eclat(transactions_1, parameter = list(supp = 0.000005, minlen = 2))
# Sort by support (frequency) in descending order and inspect the results
inspect(sort(frequent_itemsets, by = "support", decreasing = TRUE))
```

```{r}
# Run Apriori for Dataset 1
rules_1 <- apriori(transactions_1, parameter = list(supp = 0.0000001,conf=0.01))
inspect(sort(rules_1, by = "lift")[1:10])

```


#### For transaction_2:
```{r}

itemFrequencyPlot(transactions_2, topN = 20)
```




```{r}
frequent_itemsets <- eclat(transactions_2, parameter = list(supp = 0.000005, minlen = 2))
# Sort by support (frequency) in descending order and inspect the results
inspect(sort(frequent_itemsets_2, by = "support", decreasing = TRUE))
```

```{r}
# Run Apriori for Dataset 2
rules_2 <- apriori(transactions_2, parameter = list(supp = 0.0000001,conf=0.01))
inspect(sort(rules_2, by = "lift")[1:10])
```

## **Discretization for Dataset 1**
### **1. Discretizing Amount and Total Purchases (Equal-Width Binning)**
Equal-width binning divides a numeric column into bins of equal range.

```{r}
# Define number of bins
num_bins <- 4  

# Apply equal-width binning to Amount
Data_1_sliced$Amount_Bin <- cut(Data_1_sliced$Amount, breaks = num_bins, labels = FALSE, include.lowest = TRUE)

# Apply equal-width binning to Total_Purchases
Data_1_sliced$Total_Purchases_Bin <- cut(Data_1_sliced$Total_Purchases, breaks = num_bins, labels = FALSE, include.lowest = TRUE)

# View the first few rows
head(Data_1_sliced[, c("Amount", "Amount_Bin", "Total_Purchases", "Total_Purchases_Bin")])
```

---

## **Discretization for Dataset 2**
### **1. Discretizing Age (Equal-Frequency Binning)**
Equal-frequency binning ensures that each bin has approximately the same number of data points.

```{r}
# Define number of bins
num_bins <- 4

# Create equal-frequency bins for Age
Data_2_sliced$Age_Bin <- ntile(Data_2_sliced$age, num_bins)

# View the first few rows
head(Data_2_sliced[, c("age", "Age_Bin")])
```

### **2. Discretizing Transaction Hour (Custom Binning)**
We can categorize transaction hours into time-of-day groups (e.g., Morning, Afternoon, Evening, Night).

```{r}
# Define custom bins
Data_2_sliced$Transaction_Time_Bin <- cut(Data_2_sliced$transaction_hour, 
                                          breaks = c(0, 6, 12, 18, 24), 
                                          labels = c("Night", "Morning", "Afternoon", "Evening"),
                                          include.lowest = TRUE)

# View the first few rows
head(Data_2_sliced[, c("transaction_hour", "Transaction_Time_Bin")])
```

```{r}
colSums(is.na(Data_1_sliced))
```


# Clustering Analysis

## Dataset 1: K-Means Clustering

### Step 1: Select Relevant Features
```{r}
# Select relevant numerical features for clustering
data_1_cluster <- Data_1_sliced[, c("Total_Purchases", "Amount", "Total_Amount")]
```

### Step 2: Handle Missing Values
```{r}
# Remove rows with missing values
data_1_cluster <- na.omit(data_1_cluster)
```

### Step 3: Scale the Data
```{r}
# Scale numerical features for clustering
data_1_scaled <- scale(data_1_cluster)
```

### Step 4: Determine Optimal Number of Clusters using Elbow Method
```{r}
# Determine optimal number of clusters using the Elbow Method
wss <- (nrow(data_1_scaled) - 1) * sum(apply(data_1_scaled, 2, var))
wss_values <- numeric(10)
for (i in 1:10) {
  set.seed(123)
  wss_values[i] <- kmeans(data_1_scaled, centers = i, nstart = 10)$tot.withinss
}

# Plot the Elbow Method
plot(1:10, wss_values, type = "b", pch = 19, frame = FALSE, xlab = "Number of Clusters", ylab = "Total Within-Cluster Sum of Squares", main = "Elbow Method for Optimal K")
```

### Step 5: Apply K-Means Clustering

```{r}
# Apply K-Means clustering with optimal K (assuming 4 clusters)
set.seed(123)
kmeans_result_1 <- kmeans(data_1_scaled, centers = 4, nstart = 10)

# Add cluster labels to the original dataset
Data_1_sliced$Cluster <- as.factor(kmeans_result_1$cluster)
```

### Step 6: Visualize the Clusters
```{r}
# Visualize clusters using a scatter plot
library(ggplot2)
ggplot(Data_1_sliced, aes(x = Total_Purchases, y = Amount, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering for Dataset 1", x = "Total Purchases", y = "Amount")
```

---

## Dataset 2: K-Means Clustering

### Step 1: Select Relevant Features
```{r}
# Select relevant numerical features for clustering
data_2_cluster <- Data_2_sliced[, c("total_transactions", "total_sales", "avg_transaction_value")]
```

### Step 2: Handle Missing Values
```{r}
# Remove rows with missing values
data_2_cluster <- na.omit(data_2_cluster)
```

### Step 3: Scale the Data
```{r}
# Scale numerical features for clustering
data_2_scaled <- scale(data_2_cluster)
```

### Step 4: Determine Optimal Number of Clusters using Elbow Method
```{r}
# Determine optimal number of clusters using the Elbow Method
wss <- (nrow(data_2_scaled) - 1) * sum(apply(data_2_scaled, 2, var))
wss_values <- numeric(10)
for (i in 1:10) {
  set.seed(123)
  wss_values[i] <- kmeans(data_2_scaled, centers = i, nstart = 10)$tot.withinss
}

# Plot the Elbow Method
plot(1:10, wss_values, type = "b", pch = 19, frame = FALSE, xlab = "Number of Clusters", ylab = "Total Within-Cluster Sum of Squares", main = "Elbow Method for Optimal K")
```

### Step 5: Apply K-Means Clustering
```{r}
# Apply K-Means clustering with optimal K (assuming 4 clusters)
set.seed(123)
kmeans_result_2 <- kmeans(data_2_scaled, centers = 4, nstart = 10)

# Add cluster labels to the original dataset
Data_2_sliced$Cluster <- as.factor(kmeans_result_2$cluster)
```

### Step 6: Visualize the Clusters
```{r}
# Visualize clusters using a scatter plot
ggplot(Data_2_sliced, aes(x = total_transactions, y = total_sales, color = Cluster)) +
  geom_point() +
  labs(title = "K-Means Clustering for Dataset 2", x = "Total Transactions", y = "Total Sales")
```


# Decision Tree Classification

## Dataset 1: Decision Tree Model

### Step 1: Select Features
```{r}
# Select relevant features for classification
data_1_class <- Data_1_sliced[, c("Total_Purchases", "Amount", "Customer_Segment")]
```

### Step 2: Handle Missing Values
```{r}
# Remove rows with missing values
data_1_class <- na.omit(data_1_class)
```

### Step 3: Split Data into Training and Testing Sets
```{r}
library(caret)
set.seed(123)
trainIndex <- createDataPartition(data_1_class$Customer_Segment, p = 0.7, list = FALSE)
trainData_1 <- data_1_class[trainIndex, ]
testData_1 <- data_1_class[-trainIndex, ]
```

### Step 4: Train Decision Tree Model
```{r}
library(rpart)
model_1 <- rpart(Customer_Segment ~ ., data = trainData_1, method = "class")
```

### Step 5: Evaluate Model Performance

```{r}
# Convert empty strings to NA (if needed)
pred_1[pred_1 == ""] <- NA

# Remove NA values (if any)
pred_1 <- na.omit(pred_1)

# Convert to a factor
pred_1 <- factor(pred_1)


pred_1 <- factor(pred_1, levels = levels(testData_1$Customer_Segment))
# Convert empty strings to NA (if needed)
testData_1$Customer_Segment[testData_1$Customer_Segment == ""] <- NA

# Remove rows where the target variable is NA
testData_1 <- na.omit(testData_1)

# Convert to a proper factor
testData_1$Customer_Segment <- factor(testData_1$Customer_Segment)

levels(pred_1)
levels(testData_1$Customer_Segment)
```



```{r}
# Convert both to factors with matching levels
pred_1 <- factor(pred_1, levels = levels(testData_1$Customer_Segment))
testData_1$Customer_Segment <- factor(testData_1$Customer_Segment)
confusionMatrix(pred_1, testData_1$Customer_Segment)
```

### Step 6: Visualize Decision Tree
```{r}
library(rpart.plot)
rpart.plot(model_1, box.palette = "BuGn")  # Blue-Green palette
```

Becaise of the data imbalance the model is predicting only "Regular" and it is having the probability of 0.48 from the distribution but the last results are already 100 % Regular since the prediction is considering only Regular.




---

## Dataset 2: Decision Tree Model

### Step 1: Select Features
```{r}
# Select relevant features for classification
data_2_class <- Data_2_sliced[, c("total_sales", "total_transactions", "customer_city")]
```

### Step 2: Handle Missing Values
```{r}
# Remove rows with missing values
data_2_class <- na.omit(data_2_class)

# Ensure customer_city is a factor
data_2_class$customer_city <- as.factor(data_2_class$customer_city)
```

### Step 3: Split Data into Training and Testing Sets
```{r}
set.seed(123)
trainIndex_2 <- createDataPartition(data_2_class$customer_city, p = 0.7, list = FALSE)
trainData_2 <- data_2_class[trainIndex_2, ]
testData_2 <- data_2_class[-trainIndex_2, ]
```



### Step 4: Train Decision Tree Model
```{r}
model_2 <- rpart(
  customer_city ~ ., 
  data = trainData_2, 
  method = "class", 
  control = rpart.control(cp = 0.001, maxdepth = 5, minsplit = 10)
)
```

### Step 5: Evaluate Model Performance
```{r}
pred_2 <- predict(model_2, testData_2, type = "class")

# Ensure predicted values match levels of actual values
pred_2 <- factor(pred_2, levels = levels(testData_2$customer_city))

# Compute Confusion Matrix
conf_matrix <- confusionMatrix(pred_2, testData_2$customer_city)
print(conf_matrix)
```

### Step 6: Visualize Decision Tree
```{r}
rpart.plot(model_2, box.palette = "Blues", tweak = 1.2)
```


## Interpretation of the Decision Tree

### Root Node (City A - 100%)  
- The entire dataset initially belongs to **City A** (100% distribution).
- The primary splitting criterion is **total_sales ‚â• 3,597**.

### First Split (total_sales ‚â• 3,597)  
- If **total_sales ‚â• 3,597**, the classification remains **City A** (65% probability).  
- If **total_sales < 3,597**, the classification is mostly **City D** (35% probability).  

### Probabilities and Distribution  
- The numbers inside the nodes (e.g., 0.25, 0.26) represent the probability distribution across different cities.  
- The **majority class** in each node represents the most likely city assignment.  

### Key Insights  
- **Total Sales** is the most influential factor in predicting **customer_city**.  
- When **total_sales is high (‚â•3,597)**, the customer is more likely from **City A**.  
- When **total_sales is low (<3,597)**, the customer is more likely from **City D**.  
- The model is relatively simple, meaning that additional features might improve its predictive power.


